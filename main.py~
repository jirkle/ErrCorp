# coding=UTF-8
import sys
import os
import bz2
import xml.etree.ElementTree as ET
import re
import difflib
import time
import WikiExtractor
import StringIO
from multiprocessing import Pool
from collections import deque

#Czech excluded pages
excludeFilter = r".*((Hlavní\sstrana)|(Nápověda)|(Wikipedista)).*"

#Comment filters
typoFilter = r".*((typo)|(překlep)|(pravopis)|(kosmetické)|(cl)|(cleanup)|(cu)).*"
editFilter = r".*((copyedit)|(cpyed)|(ce[\s|\:])).*"
revertFilter = r".*((revert)|(rv[\s|\:])).*"

#Maximal hamming distance of two words to be considered as typo
typoTreshold = 2

#Minimal treshold that should old & new sentences from revision diff have to be considered as two similar sentences. Similarity is returned by sentenceSimilarity function.
sentenceTreshold = 0.7

#Pool processes count
poolProcesses = 8

class page:
    title = "Some interesting title"
    revisions = []

class revision:
	timestamp = "Long long time ago"
	comment = "Wonderfull improvements of content"
	text = "To be or not to be, that is the question!"
	
	def __init__(self):
		self.text = None
	
#Funkce z http://stackoverflow.com/questions/4576077/python-split-text-on-sentences, TODO: licence?
caps = "([A-Z])"
prefixes = "(Mr|St|Mrs|Ms|Dr|MUDr|JuDr|Mgr|Bc|atd|tzv|řec|lat|it|např|př)[.]"
suffixes = "(Inc|Ltd|Jr|Sr|Co)"
starters = "(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"
acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"
websites = "[.](com|net|org|io|gov|cz)"
digits = "([0-9])"
decimalPoint = "[.|,]"

def splitBySentences(text):
	if(text == None): return None
	text = " " + text + "  "
	text = text.replace("\n"," ")
	text = re.sub(prefixes,"\\1<prd>",text)
	text = re.sub(websites,"<prd>\\1",text)
	if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
	if "n.l" in text: text = text.replace("n.l.","n<prd>l<prd>")
	if "n. l" in text: text = text.replace("n. l.","n<prd> l<prd>")
	text = re.sub("\s" + caps + "[.] "," \\1<prd> ",text)
	text = re.sub(acronyms+" "+starters,"\\1<stop> \\2",text)
	text = re.sub(caps + "[.]" + caps + "[.]" + caps + "[.]","\\1<prd>\\2<prd>\\3<prd>",text)
	text = re.sub(caps + "[.]" + caps + "[.]","\\1<prd>\\2<prd>",text)
	text = re.sub(" "+suffixes+"[.] "+starters," \\1<stop> \\2",text)
	text = re.sub(" "+suffixes+"[.]"," \\1<prd>",text)
	text = re.sub(" " + caps + "[.]"," \\1<prd>",text)
	text = re.sub(digits + decimalPoint + digits,"\\1<prd>\\2",text)
	if "”" in text: text = text.replace(".”","”.")
	if "\"" in text: text = text.replace(".\"","\".")
	if "!" in text: text = text.replace("!\"","\"!")
	if "?" in text: text = text.replace("?\"","\"?")
	text = text.replace(".",".<stop>")
	text = text.replace("?","?<stop>")
	text = text.replace("!","!<stop>")
	text = text.replace("<prd>",".")
	sentences = text.split("<stop>")
	sentences = sentences[:-1]
	sentences = [s.strip() for s in sentences]
	return sentences

'''Function for text lemmatization'''
def lemma(text):
	return ''.join([i for i in text if i.isalpha() or i.isspace()]) #No lemma just remove unnecessary chars

'''Generates bag of words'''
def bagOfWords(sentence, doLemma=True):
	if(doLemma):
		sentence = lemma(sentence)
	words = re.split("\s", sentence, flags=re.MULTILINE)
	return set(words)

'''Func for distance metric (Levenshtein) used: https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python'''
def wordDistance(s1, s2):
    if len(s1) < len(s2):
        return wordDistance(s2, s1)

    # len(s1) >= len(s2)
    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer
            deletions = current_row[j] + 1       # than s2
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]



'''Generates num <0,1> which represents similarity of two given sentences'''
def sentenceSimilarity(first, second):
	firstBag = bagOfWords(first)
	secndBag = bagOfWords(second)
	similarity = 2 * float(len(firstBag & secndBag))/(len(firstBag)+len(secndBag))
	return similarity

def writeTypoToCorp(word, correction):
	print("Oprava: %1 -> %2" % (word, correction))

def getTypos(oldSent, newSent):
	oldBag = bagOfWords(oldSent)
	newBag = bagOfWords(newSent)
	difference = oldBag - newBag
	for word in difference:
		candidates = [(x, wordDistance(word, x)) for x in newBag]
		candidates = [(x, d) for (x, d) in candidates if d < typoTreshold]
		if(len(candidates) > 0):
			candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)
			writeTypoToCorp(word, candidates[0][0])

corpBuffer = []

def writeToCorpBuffer(oldSent, newSent):
	global corpBuffer
	corpBuffer.append((oldSent, newSent))

def writeCorpBuffer(comment):
	global corpBuffer
	if len(corpBuffer) > 0:
		for i in range(0, len(corpBuffer) - 1):
			if(re.search(typoFilter, comment, re.M)):
				typos = getTypos(corpBuffer[i][0], corpBuffer[i][1])
				
			#print(comment)
			print(corpBuffer[i][0])
			print(corpBuffer[i][1])
			#print("\n")
		corpBuffer = []

def processStacks(oldStack, newStack):
	if(len(oldStack) == 0 | len(newStack) == 0):
		return
	while len(oldStack) > 0:
		oldSent = oldStack.popleft()
		candidates = [(x, sentenceSimilarity(oldSent, x)) for x in newStack]
		candidates = [(x, similarity) for (x, similarity) in candidates if similarity > sentenceTreshold]
		if(len(candidates) > 0):
			candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)
			writeToCorpBuffer(oldSent, candidates[0][0])		

def processRevisions(oldRev, newRev):
	if(oldRev == None or newRev == None or oldRev.text == None or newRev.text == None):
		return
	oldStack = deque()
	newStack = deque()
	for line in difflib.unified_diff(oldRev.text, newRev.text):
			if line.startswith(' '): #Skip unnecessary output
				continue
			elif line.startswith('---'):
				continue
			elif line.startswith('+++'):
				continue
			elif line.startswith('-'): #Write diff lines from old revision to stack
				if(len(oldStack) > 0):
					processStacks(oldStack, newStack)
					newStack = deque()
					oldStack = deque()
				oldStack.append(line[1:])
			elif line.startswith('+'): #Write diff lines from new revision to stack
				if(len(newStack) > 0):
					processStacks(oldStack, newStack)
					newStack = deque()
					oldStack = deque()
				newStack.append(line[1:])
	processStacks(oldStack, newStack)

def processPage(page):
	print("Zacinam zpracovavat stranku")
	newRev = page.revisions[0]
	for i in range(1, len(page.revisions) - 1):
		oldRev = newRev
		newRev = page.revisions[i - 1]
		writeCorpBuffer(newRev.comment)
		if(i >= 2):
			revertRev = page.revisions[i - 2].comment
			if(re.search(revertFilter, revertRev, re.M)): #Look if current revision wasn't reverted
				continue
		if(re.search(editFilter, newRev.comment, re.M) or re.search(typoFilter, newRev.comment, re.M)):
			processRevisions(oldRev, newRev)
	print("Koncim zpracovani stranky")
		

def normalizeText(text):
	if(text != None):  
		out = StringIO.StringIO()
		extractor = WikiExtractor.Extractor(0, "", text.split("\n"))
		extractor.extract(out)
		text = out.getvalue()
		out.close()
		text = re.sub("==.*?==","",text) #Remove headings
		text = re.sub("\*",", ",text) #Replace bullets
		text = splitBySentences(text)
		return text
	else:
		return None

def decodeToUtf8(unicodeText):
	if(unicodeText == None): return None
	if(type(unicodeText) is str):
		return unicodeText.encode("utf-8", 'ignore')
	else:
		return unicodeText.encode("utf-8", 'ignore')

if __name__ == "__main__":
	pool = Pool(processes=poolProcesses)
	file = bz2.BZ2File("cswiki.xml.bz2", "rb")
	lines = 130801574223
	#file.seek(0, 2)
	lines = os.stat("cswiki.xml.bz2").st_size
	file.seek(0, 0)
	curPage = page()
	curRevision = revision()
	poolAsyncResults = []
	skip = False
	for event, elem in ET.iterparse(file):
		if event == 'end':
			if elem.tag.endswith('title'):
				curPage.title = decodeToUtf8(elem.text)
				if(re.search(excludeFilter, curPage.title, re.M)):
					print("Přeskakuji stránku: %s" % curPage.title)
					skip = True
					continue
				else:
					print("Zpracovávám stránku: %s" % curPage.title)
					skip = False
			if(not skip):
				if elem.tag.endswith('timestamp'):
					curRevision.timestamp = decodeToUtf8(elem.text)
				elif elem.tag.endswith('comment'):
					curRevision.comment = decodeToUtf8(elem.text)
				elif elem.tag.endswith('text'):
					#try:
						curRevision.text = normalizeText(elem.text)
						#poolAsyncResults.append(pool.apply_async(normalizeText, args=(elem.text,)))
					#except:
					#	curRevision.text = None
				elif elem.tag.endswith('revision'):
					curPage.revisions.append(curRevision)
					curRevision = revision()
				elif elem.tag.endswith('page'):
					pool.close()					
					'''for i in range(0, len(poolAsyncResults) - 1):
						poolAsyncResults[i].ready()
						curPage.revisions[i].text = poolAsyncResults[i].get()	#collect results from pool'''
					#pool.apply(processPage, args=(curPage,))
					processPage(curPage)
					curPage = page()
					pool.join()
					pool = Pool(processes=poolProcesses)
			elem.clear()
			